{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前在的机器是： cpu\n"
     ]
    }
   ],
   "source": [
    "# Transformer 配置参数\n",
    "# GPU device setting\n",
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('当前在的机器是：', device)\n",
    "# 模型参数\n",
    "batch_size = 128 # 训练批次 句话\n",
    "max_len = 256    # 单句最大长度 \n",
    "##\n",
    "# padding=10\n",
    "\n",
    "d_model = 512    # 词嵌入向量维度\n",
    "n_layers = 6     # encoder/decoder层数量\n",
    "n_heads = 8      # 注意力头数： 假如有词嵌入维度d_model = 512 / n_heads = 8 => 单头向量维度 512 / 8 = 64，即QKV维度\n",
    "ffn_hidden = 2048 # 前向传播维度。 512 -> 2048 -> 512, 通常也称作proj\n",
    "drop_prob = 0.1  # dropout提升鲁棒性，随机失活一些节点\n",
    "n_hidden = ffn_hidden\n",
    "\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 100\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load src shape torch.Size([128, 27])\n",
      "load trg shape torch.Size([128, 28])\n"
     ]
    }
   ],
   "source": [
    "src_pad_idx = 1\n",
    "trg_pad_idx = 1\n",
    "trg_sos_idx = 2\n",
    "enc_voc_size = 5893\n",
    "dec_voc_size = 7853\n",
    "\n",
    "test_src = torch.load('./data/tensor_src.pt')\n",
    "test_trg = torch.load('./data/tensor_trg.pt')\n",
    "print(\"load src shape\", test_src.shape)\n",
    "print(\"load trg shape\", test_trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([14, 512])\n",
      "输入数据 torch.Size([3, 10])\n",
      "输入数据的embedding torch.Size([3, 10, 512])\n",
      "tensor([ 1.5583,  1.1276, -0.7070, -0.9933, -0.8856,  1.0061, -0.1717, -0.2618,\n",
      "         0.9074,  0.4190,  0.9231, -0.2430], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "embd_layer = torch.nn.Embedding(14, 512)\n",
    "print('embedding.weight', embd_layer.weight.shape)\n",
    "\n",
    "\n",
    "input_id = torch.tensor([[2, 4, 5, 6, 7, 8, 3, 1, 1, 1], \n",
    "                      [2, 4, 9, 10,11,12,13,3, 1, 1],\n",
    "                      [2, 6, 7, 8, 9, 10,11,12,13,3]])\n",
    "\n",
    "\n",
    "print(\"输入数据\",input_id.shape)\n",
    "print(\"输入数据的embedding\", embd_layer(input_id).shape)\n",
    "\n",
    "print(embd_layer(input_id)[0][1][:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5893, 512])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "a = nn.Embedding(enc_voc_size, d_model)\n",
    "# embedding_layer = nn.Embedding(14, 128)\n",
    "print(a.weight.shape) # 14 * 128\n",
    "print(input_id.shape) # \n",
    "x = a(input_id)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenEmbedding(5893, 512, padding_idx=1)\n",
      "TokenEmbedding(7853, 512, padding_idx=1)\n"
     ]
    }
   ],
   "source": [
    "# 创建Token embedding类\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "        \n",
    "test_src_token = TokenEmbedding(enc_voc_size, d_model) #对 src：en 进行embedding\n",
    "test_trg_token = TokenEmbedding(dec_voc_size, d_model) #对 trg：de 进行embedding\n",
    "print(test_src_token) \n",
    "print(test_trg_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 位置编码position encoding\n",
    "pos代表max_len里的位置, i代表d_model里的维度\n",
    "$$PE_{pos, 2i} =  sin(pos / 10000^{2i / d})$$\n",
    "$$PE_{pos, 2i+1} = cos(pos / 10000^{2i /d})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 512])\n",
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device) # [256, 512]\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        pos = torch.arange(0, max_len, device=device) # torch.size([256])\n",
    "        pos = pos.float().unsqueeze(dim=1) # 增加一个维度[256, 1]\n",
    "        _2i = torch.arange(0, d_model, 2, device=device) # torch.size([256])\n",
    "\n",
    "        # pos / (10000 ** (_2i / d_model))的维度是[256, 256]\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "    \n",
    "    # x.shape: [batch_size, seq_len]\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]\n",
    "\n",
    "test_pos_encoding = PositionalEncoding(d_model, max_len, device)\n",
    "print(test_pos_encoding.encoding.shape)\n",
    "print(test_pos_encoding.encoding[0:10,:].shape) # 255 is position "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LayerNorm\n",
    "$$LayerNorm(x) = α * \\frac{(x-mean)} {\\sqrt{var+ eps}}  + β$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 512])\n",
      "torch.Size([2, 4, 512])\n",
      "tensor([[[-0.9189,  1.4861,  0.7582,  ..., -0.3212,  1.7563, -0.9268],\n",
      "         [-0.3603, -0.2978, -0.1365,  ..., -0.8413,  0.3929, -0.3002],\n",
      "         [-0.4081,  1.7437,  0.8381,  ...,  0.4693, -0.9644,  1.3948],\n",
      "         [-1.3597, -1.6293, -1.1009,  ..., -1.0459,  0.6226, -1.3483]],\n",
      "\n",
      "        [[-0.8746, -0.0809,  0.1776,  ..., -1.2659,  1.2809,  1.6349],\n",
      "         [ 0.2639, -0.6621, -1.4123,  ...,  1.4496,  1.0469,  1.2344],\n",
      "         [ 1.2481,  1.4071, -0.3268,  ..., -1.1881, -1.0544, -1.2130],\n",
      "         [-0.8888, -0.8657, -0.8292,  ...,  0.4949,  0.9049, -0.5779]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "d_model = 512\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layernorm作用在(-1) 最后一维进行归一化\n",
    "        mean = x.mean(dim=-1, keepdim=True) # [2, 4, 1]\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)# [2, 4, 1]\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)# [2, 4, 512]\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 单头注意力机制\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len, d_model = k.size()\n",
    "        # d_model是标量，用math.sqrt()\n",
    "        score = q * k.transpose(2, 3) / math.sqrt(d_model)\n",
    "        if not mask:\n",
    "            score = score.masked_fill(mask==0, -10000)\n",
    "        attn = self.softmax(score) @ v \n",
    "        return attn, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. FFN\n",
    "$$FFN(x) = ln2(relu(ln1(x)))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionwiseFeedForward(\n",
      "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, d_model)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "ffw = PositionwiseFeedForward(d_model, ffn_hidden)\n",
    "print(ffw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. mutli-head attention\n",
    "$$MutilHead(Q, K, V) = Concat(head_1, ..., head_h) W^O$$\n",
    "$$head_i = Attention(Q{W_i}^Q, K{W_i}^K, V{W_i}^V)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (attention): ScaleDotProductAttention(\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "512 8\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        batch_size, seq_len, d_model = q.size()\n",
    "        q = q.view(batch_size, -1, self.n_head, d_model).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.n_head, d_model).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.n_head, d_model).transpose(1, 2)\n",
    "        attn, _score = self.attention(q, k, v, mask=mask)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        attn = self.w_o(attn)\n",
    "        return attn\n",
    "        \n",
    "test_multihead_attention = MultiHeadAttention(d_model, n_heads)\n",
    "print(test_multihead_attention)\n",
    "print(d_model, n_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
